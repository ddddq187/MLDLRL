{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hrl.boyuai.com/chapter/1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P=np.array(P)\n",
    "rewards=np.array([-1,-2,-2,10,1,0])\n",
    "gamma=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_rewards(start_index,chain,gamma):\n",
    "    G=0\n",
    "    for i in reversed(range(start_index,len(chain))):\n",
    "        G=rewards[chain[i]-1]+G*gamma\n",
    "    return G\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 6]的回报为-2.5\n"
     ]
    }
   ],
   "source": [
    "chain=[1,2,3,6]\n",
    "start_index=0\n",
    "print(f'{chain}的回报为{path_rewards(start_index,chain,gamma)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于价值函数\n",
    "$E[G_t|S_t=s]$表示的是在t时刻状态是s，从s出发的价值期望\n",
    "$G_t=R_t+\\gamma *R_{t+1}+\\gamma ^2 *R_{t+2}\\cdots$\n",
    "\n",
    "$G_t$写成Rt加上$\\gamma R_t+1$\n",
    "\n",
    "那么我么可以写成从状态s（时间t）转移到s‘（t+1）然后每个转移概率乘上s’的价值函数\n",
    "\n",
    "$$V(s)=r(s)+\\gamma \\sum_{s^{\\prime} \\in S} p\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right)$$\n",
    "\n",
    "写成向量形式\n",
    "Bellman equation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{v} &=\\mathbf{R}+\\gamma \\mathbf{P} \\mathbf{v} \\\\\n",
    "{\\left[\\begin{array}{c}\n",
    "V\\left(s_{1}\\right) \\\\\n",
    "V\\left(s_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "V\\left(s_{n}\\right)\n",
    "\\end{array}\\right] } &=\\left[\\begin{array}{c}\n",
    "r\\left(s_{1}\\right) \\\\\n",
    "r\\left(s_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "r\\left(s_{n}\\right)\n",
    "\\end{array}\\right]+\\gamma\\left[\\begin{array}{cccc}\n",
    "P\\left(s_{1} \\mid s_{1}\\right) & P\\left(s_{2} \\mid s_{1}\\right) & \\cdots & P\\left(s_{n} \\mid s_{1}\\right) \\\\\n",
    "P\\left(s_{1} \\mid s_{2}\\right) & P\\left(s_{2} \\mid s_{2}\\right) & \\cdots & P\\left(s_{n} \\mid s_{2}\\right) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P\\left(s_{1} \\mid s_{n}\\right) & P\\left(s_{2} \\mid s_{n}\\right) & \\cdots & P\\left(s_{n} \\mid s_{n}\\right)\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "V\\left(s_{1}\\right) \\\\\n",
    "V\\left(s_{2}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "V\\left(s_{n}\\right)\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(P,rewards,gamma,states_num):\n",
    "    rewards=rewards.reshape((-1,1))\n",
    "    value=np.dot(np.linalg.inv(np.eye(states_num,states_num)-gamma*P),rewards)\n",
    "    #value=np.linalg.inv(np.eye(states_num,states_num)-gamma*P)@rewards\n",
    "    #两种实现\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.01950168],\n",
       "       [-2.21451846],\n",
       "       [ 1.16142785],\n",
       "       [10.53809283],\n",
       "       [ 3.58728554],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V=compute(P,rewards,gamma,states_num=6)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上边是马尔可夫奖励过程,都是自发的\n",
    "加上外部动作集合\n",
    "则成为马尔可夫决策过程（S，A，P，gamma，r）\n",
    "\n",
    "策略\n",
    "$\\pi(a|s)=P(A_t=a|S_t=s)$\n",
    "\n",
    "表示在输入状态情况s下采取动作a的概率\n",
    "\n",
    "状态价值函数\n",
    "\n",
    "我们用 $V^{\\pi}(s)$ 表示在 MDP 中基于策略 $\\pi$ 的状态价值函数（state - value function），定义为从状态 $s$ 出发遵循策略 $\\pi$ 所能获得的期望回报，数学表达式为：\n",
    "$$V^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_{t}\\vert S_{t}=s]$$\n",
    "\n",
    "动作价值函数\n",
    "\n",
    "用 $Q^{\\pi}(s,a)$ 表示在 MDP 遵循策略 $\\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报：\n",
    "$$Q^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_{t}\\vert S_{t}=s,A_{t}=a]$$\n",
    "\n",
    "显然 两个函数的关系是\n",
    "\n",
    "$$V^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)Q^{\\pi}(s,a)$$\n",
    "\n",
    "同理，贝尔曼期望方程是显然的，只不过都带上了动作\n",
    "\n",
    "\n",
    "$$Q^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^{\\pi}(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "# 奖励函数\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)\n",
    "\n",
    "# 策略1,随机策略\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "# 策略2\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们想要计算该MDP下，一个策略π的状态价值函数。我们现在有的工具是MRP的解析解方法。于是，一个很自然的想法是：给定一个MDP和一个策略π，我们是否可以将其转化为一个MRP？答案是肯定的。我们可以将策略的动作选择进行边缘化（marginalization），就可以得到没有动作的MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即：\n",
    "$$r^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)r(s,a)$$\n",
    "同理，我们计算采取动作的概率与使s转移到s'的概率的乘积，将这些乘积相加，其和就是一个MRP的状态从s转移至s'的概率：\n",
    "$$P^{\\pi}(s'|s)=\\sum_{a\\in A}\\pi(a|s)P(s'|s,a)$$\n",
    "于是，我们构建得到了一个MRP:$\\langle S,P^{\\pi},r^{\\pi},\\gamma\\rangle$。根据价值函数的定义可以发现，转化后的MRP的状态价值函数是一样的。于是我们可以用MRP状态价值函数的解析解来计算这个MDP中该策略的状态价值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP中每个状态价值分别为\n",
      " [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "# 转化后的MRP的状态转移矩阵\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
    "R_from_mdp_to_mrp = np.array([-0.5, -1.5, -1.0, 5.5, 0])\n",
    "\n",
    "V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\n",
    "print(\"MDP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo methods\n",
    "例如在正方形和园的图形中随机打点，用点的个数比例近似面积比例\n",
    "\n",
    "我们现在介绍如何用蒙特卡洛方法来估计策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在MDP上采样很多条序列，计算从状态出发的回报再求其期望就可以了，公式如下：\n",
    "$$V^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_{t}\\vert S_{t}=s]\\approx\\frac{1}{N}\\sum_{i = 1}^{N}G_{t}^{(i)}$$\n",
    "在一条用策略采样得到的序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的期望回报，也就是这条序列只计算一次该状态的期望回报，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略π采样若干条序列：\n",
    "(1) 使用策略π采样若干条序列；\n",
    "(2) 对每一条序列中的每一时间步t的状态s进行以下操作：\n",
    "   - 更新状态s的计数器\n",
    "   $$N(s)\\leftarrow N(s)+1$$\n",
    "   - 更新状态s的总回报\n",
    "   $$M(s)\\leftarrow M(s)+G_{t}$$\n",
    "(3) 每一个状态的价值被估计为回报的平均值\n",
    "   $$V(s)=M(s)/N(s)$$\n",
    "根据大数定律，当$$N(s)\\rightarrow\\infty$$，有$$V(s)\\rightarrow V^{\\pi}(s)$$。计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G，进行如下计算：\n",
    "   - $$N(s)\\leftarrow N(s)+1$$\n",
    "   - $$V(s)\\leftarrow V(s)+\\frac{1}{N(s)}(G - V(s))$$\n",
    "\n",
    "这种增量式更新方式在老虎机中已经介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条序列\n",
      " [('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n",
      "第二条序列\n",
      " [('s4', '概率前往', 1, 's4'), ('s4', '前往s5', 10, 's5')]\n",
      "第五条序列\n",
      " [('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n"
     ]
    }
   ],
   "source": [
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestep = 0\n",
    "        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n",
    "        # 当前状态为终止状态或者时间步太长时,一次采样结束\n",
    "        while s != \"s5\" and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 在状态s下根据策略选择动作\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s, a_opt), 0)#字典的get方法，返回key的value，但是如果没有匹配到key，就返回0\n",
    "                if temp > rand:\n",
    "                    a = a_opt\n",
    "                    r = R.get(join(s, a), 0)\n",
    "                    break\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 根据状态转移概率得到下一个状态s_next\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s, a), s_opt), 0)#temp累计是因为：Inverse transform sampling 利用累积分布函数依赖于从均匀分布中抽取的随机数来产生符合期望概率分布的随机变量。可以搜索小时百科的马尔科夫链蒙特卡洛深入了解\n",
    "                if temp > rand:\n",
    "                    s_next = s_opt\n",
    "                    break\n",
    "            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n",
    "            s = s_next  # s_next变成当前状态,开始接下来的循环\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "\n",
    "# 采样5次,每个序列最长不超过20步\n",
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('第一条序列\\n', episodes[0])\n",
    "print('第二条序列\\n', episodes[1])\n",
    "print('第五条序列\\n', episodes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用蒙特卡洛方法计算MDP的状态价值为\n",
      " {'s1': -1.228923788722258, 's2': -1.6955696284402704, 's3': 0.4823809701532294, 's4': 5.967514743019431, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "def MC(episodes,V,N,gamma):\n",
    "    for episode in episodes:\n",
    "        G=0\n",
    "        for i in reversed(range(0,len(episode))):\n",
    "            #range(len(episode) - 1, -1, -1): 从len-1开始，-1结束（不包括-1），步进-1\n",
    "            (s,a,r,s_next)=episode[i]\n",
    "            G=r+gamma*G\n",
    "            N[s]=N[s]+1\n",
    "            V[s]=V[s]+(G-V[s])/N[s]\n",
    "\n",
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes, V, N, gamma)\n",
    "print(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "占有度量\n",
    "\n",
    "不同策略会使智能体访问到不同概率分布的状态,影响到策略的价值函数\n",
    "\n",
    "首先我们定义MDP的初始状态分布为$\\nu_{0}(s)$，然后就可以定义一个策略的状态访问分布（state visitation distribution）：\n",
    "\n",
    "我们用$P_{t}^{\\pi}(s)$表示采取策略$\\pi$使得智能体在$t$时刻状态为$s$的概率，所以我们有$P_{0}^{\\pi}(s)=\\nu_{0}(s)$：\n",
    "\n",
    "$$\n",
    "\\nu^{\\pi}(s)=(1 - \\gamma)\\sum_{t = 0}^{\\infty}\\gamma^{t}P_{t}^{\\pi}(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1-\\gamma$是使得概率和为1的归一化因子，以下性质：\n",
    "\n",
    "$v^{\\pi}(s')=(1 - \\gamma)v_0(s') + \\gamma \\int \\int P(s'|s,a)\\pi(a|s)v^{\\pi}(s)dsda$\n",
    "\n",
    "也可以定义策略的占用度量：\n",
    "\n",
    "\n",
    "\n",
    "$$\\rho^{\\pi}(s,a)=(1 - \\gamma)\\sum_{t = 0}^{\\infty}\\gamma^{t}P_{t}^{\\pi}(s)\\pi(a|s)$$\n",
    "\n",
    "$$\\rho^{\\pi}(s,a)=\\nu^{\\pi}(s)\\pi(a|s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定理一\n",
    "智能体分别以策略$\\pi_1$和$\\pi_2$和同一个MDP交互得到的占用度量$\\rho^{\\pi_1}$和$\\rho^{\\pi_2}$满足\n",
    "\n",
    "$ \\rho^{\\pi_1} = \\rho^{\\pi_2} \\Leftrightarrow \\pi_1 = \\pi_2 $\n",
    "\n",
    "定理二\n",
    "\n",
    "给定唯一合法的占用度量$\\rho$,生成占用度量的唯一策略是：\n",
    "\n",
    "$\\pi_{\\rho} = \\frac{\\rho(s,a)}{\\sum_{a'}\\rho(s,a')}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupancy(eposides,s,a,timestep_max,gamma):\n",
    "    #利用s，a对的频率估计占用度量\n",
    "    rho=0\n",
    "    total_times=np.zeros(timestep_max)\n",
    "    occu_times=np.zeros(timestep_max)\n",
    "    for eposide in eposides:\n",
    "        for i in range(len(eposide)):\n",
    "            (s_opt,a_opt,r,s_next)=eposide[i]\n",
    "            total_times[i]+=1\n",
    "            if s == s_opt and a == a_opt:\n",
    "                occu_times[i] += 1\n",
    "    for i in reversed(range(timestep_max)):\n",
    "        if total_times[i]:\n",
    "            rho += gamma**i*occu_times[i]/total_times[i]\n",
    "    return (1-gamma)*rho\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11311079413299546 0.2443171811390214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
    "rho_1 = occupancy(episodes_1, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "rho_2 = occupancy(episodes_2, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "print(rho_1, rho_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优策略\n",
    "\n",
    "$\n",
    "\\pi > \\pi' \\text{ 定义偏序关系，当且仅当对于任意的状态 } s \\text{ 都有 } V^{\\pi}(s) \\geq V^{\\pi'}(s)\n",
    "$\n",
    "\n",
    "最优策略都有相同的状态价值函数，我们称之为最优状态价值函数\n",
    "\n",
    "$V^*(s) = \\max_{\\pi} V^{\\pi}(s), \\ \\forall s \\in S$\n",
    "\n",
    "同理，我们定义最优动作价值函数:\n",
    "\n",
    "$Q^*(s, a) = \\max_{\\pi} Q^{\\pi}(s, a), \\ \\forall s \\in S, a \\in A$\n",
    "\n",
    "为了使Q最大，我们需要在当前的状态动作对sa之后都执行最优策略。于是我们得到了最优状态价值函数和最优动作价值函数之间的关系：\n",
    "\n",
    "$Q^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in S}P(s'|s,a)V^*(s')$\n",
    "\n",
    "另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值\n",
    "\n",
    "$V^*(s) = \\max_{a \\in A} Q^*(s,a)$\n",
    "\n",
    "\n",
    "得到贝尔曼最优方程（Bellman optimality equation）\n",
    "\n",
    "$\n",
    "V^*(s)=\\max_{a\\in A}\\{r(s,a)+\\gamma\\sum_{s'\\in S}p(s'|s,a)V^*(s')\\}\n",
    "$\n",
    "\n",
    "$\n",
    "Q^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in S}p(s'|s,a)\\max_{a'\\in A}Q^*(s',a')\n",
    "$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDLRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
